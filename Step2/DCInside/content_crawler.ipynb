{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa19f005",
   "metadata": {},
   "source": [
    "# Content Crawler\n",
    "\n",
    "This crawler will be the next step of `list_crawler.ipynb`.\n",
    "\n",
    "When given a csv file, such as `kpop_agenda/Step2/DCInside/dcinside_마이카_list.csv`,\n",
    "\n",
    "It will crawl through all the URLs in the `URL` column of the csv file, then store the article contents in a text file.\n",
    "\n",
    "The folder location where the text files will be stored is `kpop_agenda/Step2/DCInside/Articles`.\n",
    "\n",
    "The file names for the articles will be stored as `{keyword}-YYYYMMDD-HHMM-{number}.txt` if there is another article with the same keyword at the same minute, then we use the number at the end. Usually it will be 1, but sometimes they may have been multiple articles at the same minute.\n",
    "\n",
    "Also, if there are multiple articles with the exact same title within 10 minutes, we consider that as one person \"spamming\" across many galleries, and we ignore them except for the initial one.\n",
    "\n",
    "Lastly, we update the `kpop_agenda/Step2/DCInside/dcinside_마이카_list.csv` csv file by adding a column at the end, `location`. This is where the article text file is located at, like `kpop_agenda/Step2/DCInside/Articles/마이카-20251228-0112-1.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import aiohttp\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ac2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_PATH = 'C:/Users/WINDOWS 11/Desktop/kpop_agenda/Step2/DCInside/dcinside_마이카_list.csv'\n",
    "OUTPUT_DIR = 'C:/Users/WINDOWS 11/Desktop/kpop_agenda/Step2/DCInside/Articles'\n",
    "CONCURRENT_REQUESTS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 230 rows. Pre-processing for spam and filenames...\n",
      "Filtered down to 230 valid articles. Starting async crawl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 230/230 [00:46<00:00,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed! 230 articles successfully saved.\n",
      "Updated CSV saved to C:/Users/WINDOWS 11/Desktop/kpop_agenda/Step2/DCInside/dcinside_마이카_list.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "async def fetch_and_save(session, row, semaphore):\n",
    "    url = row['URL']\n",
    "    file_path = row['target_path']\n",
    "    \n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url, headers=HEADERS, timeout=15) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    \n",
    "                    # 1. Extract Content\n",
    "                    content_div = soup.find('div', class_='write_div')\n",
    "                    if not content_div:\n",
    "                        content_div = soup.find('div', class_='thum-txt') # Fallback\n",
    "                    \n",
    "                    if content_div:\n",
    "                        content = content_div.get_text('\\n', strip=True)\n",
    "                        \n",
    "                        # 2. Clean Content (\"- dc official App\" removal)\n",
    "                        lines = content.split('\\n')\n",
    "                        if lines and lines[-1].strip() == \"- dc official App\":\n",
    "                            lines.pop()\n",
    "                            content = '\\n'.join(lines)\n",
    "\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(content)\n",
    "                        \n",
    "                        return True\n",
    "                    else:\n",
    "                        print(f\"  [Warning] No content found: {url}\")\n",
    "                        return False\n",
    "                else:\n",
    "                    print(f\"  [Error] Status {response.status}: {url}\")\n",
    "                    return False\n",
    "        except Exception as e:\n",
    "            print(f\"  [Exception] {url}: {e}\")\n",
    "            return False\n",
    "\n",
    "async def main():\n",
    "    if not os.path.exists(INPUT_CSV_PATH):\n",
    "        print(f\"Error: File not found at {INPUT_CSV_PATH}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(INPUT_CSV_PATH)\n",
    "    print(f\"Loaded {len(df)} rows. Pre-processing for spam and filenames...\")\n",
    "\n",
    "    # Filter spam & generate filenames\n",
    "    df['dt'] = pd.to_datetime(df['time'], format='%Y.%m.%d %H:%M')\n",
    "    df = df.sort_values('dt', ascending=True)\n",
    "\n",
    "    valid_rows = []\n",
    "    seen_titles = {}\n",
    "    file_counters = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        current_dt = row['dt']\n",
    "        keyword = row['keyword']\n",
    "\n",
    "        if title in seen_titles:\n",
    "            last_seen_dt = seen_titles[title]\n",
    "            if (current_dt - last_seen_dt) <= timedelta(minutes=3): # Might have to adjust this time\n",
    "                continue\n",
    "        seen_titles[title] = current_dt\n",
    "\n",
    "        time_str = current_dt.strftime('%Y%m%d-%H%M')\n",
    "        key = (keyword, time_str)\n",
    "        file_counters[key] = file_counters.get(key, 0) + 1\n",
    "        \n",
    "        filename = f\"{keyword}-{time_str}-{file_counters[key]}.txt\"\n",
    "        file_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        \n",
    "        row['target_path'] = file_path\n",
    "        row['location'] = file_path\n",
    "        valid_rows.append(row)\n",
    "\n",
    "    print(f\"Filtered down to {len(valid_rows)} valid articles. Starting crawl...\")\n",
    "\n",
    "    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)\n",
    "    tasks = []\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for row in valid_rows:\n",
    "            task = asyncio.create_task(fetch_and_save(session, row, semaphore))\n",
    "            tasks.append(task)\n",
    "        \n",
    "        try:\n",
    "            from tqdm.asyncio import tqdm\n",
    "            results = await tqdm.gather(*tasks)\n",
    "        except ImportError:\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            print(\"Finished crawling.\")\n",
    "\n",
    "    final_df = pd.DataFrame(valid_rows)\n",
    "    \n",
    "    if 'dt' in final_df.columns:\n",
    "        final_df = final_df.drop(columns=['dt'])\n",
    "    if 'target_path' in final_df.columns: \n",
    "        final_df = final_df.drop(columns=['target_path'])\n",
    "\n",
    "    final_df.to_csv(INPUT_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nCompleted. {sum(results)} articles successfully saved.\")\n",
    "    print(f\"Updated CSV saved to {INPUT_CSV_PATH}.\")\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
