{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "JDK is required to run this code, as it is required for `konlpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WINDOWS 11\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "import os\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"C:/Users/WINDOWS 11/Desktop/kpop_agenda/data/articles_metadata.csv\"\n",
    "articles_dir = \"C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20241223_article_4.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20250320_article_2.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20250320_article_6.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20250331_article_4.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20250605_article_2.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20250606_article_5.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20250816_article_1.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20251026_article_7.txt\n",
      "FAILED to find: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data\\20251027_article_1.txt\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "\n",
    "articles_ner_info = {}\n",
    "article_ids = []\n",
    "\n",
    "def extract_ner_information(article_content):\n",
    "    pos_tags = okt.pos(article_content, norm=True, stem=True)\n",
    "    person_entities = set()\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tags:\n",
    "        if tag == \"Noun\":\n",
    "            cleaned_tokens.append(token)\n",
    "            if len(token) > 1:\n",
    "                person_entities.add(token)\n",
    "    return {\n",
    "        \"person_entities\": person_entities,\n",
    "        \"cleaned_article_text\": \" \".join(cleaned_tokens)\n",
    "    }\n",
    "\n",
    "print(f\"Looking for files in: {articles_dir}\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    raw_filename = str(row['file_path'])\n",
    "    clean_filename = os.path.basename(raw_filename)\n",
    "    \n",
    "    article_id = row['ID']\n",
    "    full_file_path = os.path.join(articles_dir, clean_filename)\n",
    "\n",
    "    try:\n",
    "        with open(full_file_path, 'r', encoding='utf-8') as f:\n",
    "            article_content = f.read()\n",
    "            ner_info = extract_ner_information(article_content)\n",
    "            articles_ner_info[article_id] = ner_info\n",
    "            article_ids.append(article_id)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"FAILED to find: {full_file_path}\")\n",
    "        articles_ner_info[article_id] = {\"person_entities\": set(), \"cleaned_article_text\": \"\"}\n",
    "        article_ids.append(article_id)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Embeddings (using cleaned noun-based text)\n",
    "model = SentenceTransformer('all-mpnet-base-v2') # all-mpnet-base-v2 for quality performance\n",
    "cleaned_articles_for_embedding = [articles_ner_info[id_]['cleaned_article_text'] for id_ in article_ids]\n",
    "embeddings = model.encode(cleaned_articles_for_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Similarity Matrix\n",
    "similarity_matrix = cosine_similarity(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ner_similarity(article_id1, article_id2, embedding_similarity):\n",
    "    ner_info1 = articles_ner_info[article_id1]\n",
    "    ner_info2 = articles_ner_info[article_id2]\n",
    "\n",
    "    person_entity_overlap = len(ner_info1[\"person_entities\"].intersection(ner_info2[\"person_entities\"]))\n",
    "\n",
    "    person_entity_diff_penalty = 0\n",
    "    if ner_info1[\"person_entities\"] and ner_info2[\"person_entities\"]:\n",
    "        person_entity_diff_ratio = 1 - (person_entity_overlap / min(len(ner_info1[\"person_entities\"]), len(ner_info2[\"person_entities\"])))\n",
    "        person_entity_diff_penalty = person_entity_diff_ratio * 0.93\n",
    "\n",
    "    refined_similarity = embedding_similarity - person_entity_diff_penalty  # Removed relationship keyword bonus\n",
    "\n",
    "    return max(0, refined_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph-Based Clustering with NER-aware Similarity\n",
    "threshold = 0.509  # Adjust threshold\n",
    "graph = nx.Graph()\n",
    "\n",
    "for i in range(len(article_ids)):\n",
    "    id1 = article_ids[i]\n",
    "    graph.add_node(id1)\n",
    "    for j in range(i + 1, len(article_ids)):\n",
    "        id2 = article_ids[j]\n",
    "        embedding_similarity = similarity_matrix[i, j] # Get embedding similarity\n",
    "        refined_similarity = calculate_ner_similarity(id1, id2, embedding_similarity) # Calculate NER-aware similarity\n",
    "\n",
    "        if refined_similarity > threshold:\n",
    "            graph.add_edge(id1, id2, weight=refined_similarity) # You can store similarity as edge weight\n",
    "\n",
    "connected_components = list(nx.connected_components(graph))\n",
    "\n",
    "unique_ids = {}\n",
    "next_unique_id = 1\n",
    "\n",
    "for component in connected_components:\n",
    "    for article_id in component:\n",
    "        unique_ids[article_id] = next_unique_id\n",
    "    next_unique_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"unique_article_ID\" column to DataFrame\n",
    "df['unique_article_ID'] = df['ID'].map(unique_ids) # efficient way to add the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated CSV file saved to: C:/Users/WINDOWS 11/Desktop/kpop_agenda/data/articles_metadata_deduplicated.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the updated CSV file\n",
    "output_csv_file = \"C:/Users/WINDOWS 11/Desktop/kpop_agenda/data/articles_metadata_deduplicated.csv\" # Change the output path\n",
    "df.to_csv(output_csv_file, encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"Deduplicated CSV file saved to: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the unique ID to find clusters of similar articles\n",
    "clusters = df.groupby('unique_article_ID')\n",
    "\n",
    "print(\"Similar Content Clusters:\")\n",
    "found_duplicates = False\n",
    "\n",
    "for cluster_id, group in clusters:\n",
    "    # If the group has more than 1 entry, it means they are considered 'same content'\n",
    "    if len(group) > 1:\n",
    "        found_duplicates = True\n",
    "        print(f\"\\nCluster {cluster_id}:\")\n",
    "        for idx, row in group.iterrows():\n",
    "            # Prints the ID and the filename for comparison\n",
    "            print(f\" - [ID: {row['ID']}] {row['file_path']}\")\n",
    "\n",
    "if not found_duplicates:\n",
    "    print(\"No duplicate clusters found with the current threshold.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
